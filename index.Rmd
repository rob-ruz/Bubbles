---
title: "Caso"
author: "Roberto Ruz Campos"
date: "30/11/2019"
output:
  html_document:
    df_print: paged
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(janitor)
library(readxl)
library(tsibble)
library(rlang)
```

Se importa la tabla "Compustat Global Daily" con los tipos de datos correctos.
```{r}
global_daily <- read_csv("Compustat_Global_Daily.csv",
  col_types = cols(
    sedol = col_character(),
    datadate = col_date(format = "%Y%m%d")
  )
)
```

Se agregan el sector correspondiente a cada empresa según la clasificación de la BMV
<p style="color:red">
**NOTA: Hay que tener en la carpeta local del proyecto el archivo "Sectores_GICS_BMV"**
</p>

```{r}
global_daily <- read_xlsx(
  path = "Sectores_GICS_BMV.xlsx",
  sheet = "Sectores",
  range = "A1:D292",
  col_names = TRUE,
  col_types = c("text")
) %>%
  select(gvkey, BMV) %>%
  left_join(
    global_daily,
    .,
    by = "gvkey"
  )
```

## Filtros de país y moneda. 

De la base original extraemos un subconjunto de datos. Este subconjunto corresponde a los registros con valor 208 en la columna "exchg".
```{r}
#Creamos un nuevo data frame, en caso de querer regresar a este punto.
DatosMX01 <- global_daily %>%
  filter(
    exchg == 208,
    !curcdd %in% "USD"
  )

# Generamos un resumen para ver los resultados del filtro.
DatosMX01 %>%
  group_by(exchg) %>%
  count(curcdd)
# Solo quedan registros en MXP, MXN, NA    

# Revisar las características de los registros que en el campo "curcdd" tienen "NA" e identificar si estos registros tienen información de precios.
DatosMX01 %>%
  summarise_all(~ sum(is.na(.)))

DatosMX01 %>% 
  filter(is.na(curcdd))

# El resumen muestra que los registros que en el campo "curcdd" tienen "NA", no tienen registro de precios.Por lo tanto los eliminamos de "DatosMX01".
DatosMX01 <- DatosMX01 %>%
  filter(!is.na(curcdd))

```

## Duplicidad de fechas

Es importante tener en cuenta que un `GVKEY` es un número único asignado a cada compañía en la base de datos de Compustat (ver: https://libguides.stanford.edu/c.php?g=559845&p=6686228). El `GVKEY` puede ser utilizado en lugar del `CUSIP`, un identificador equivalente al `ISIN`. Asimismo, el `Issue ID` (`iid`) es un código alfanumérico único para cada emisión de valores de una compañía (`GVKEY`) (ver: https://library.unist.ac.kr/libguide/wp-content/uploads/sites/2/2018/11/compustat.pdf)

En el código del siguiente chunk, analizamos la relación entre `GVKEY` e `isin`

```{r}
# Generamos un identificador único concatenando el campo "gvkey" con "iid".
DatosMX01 <- DatosMX01 %>% 
  mutate(gvkey_iid = paste(gvkey, iid, sep = "_")) %>% 
  select(gvkey_iid, everything())


# Generamos un resumen para identificar la cantidad de distintos `isin` asociados a un único "GVKEY_IID".
# Notemos que existen algunas emisiones (gvkey_iid) que no cuentan con isin
DatosMX01 %>%
  group_by(conm, gvkey_iid, isin) %>%
  summarise(
    distinct_isin = n_distinct(isin),
    count_isin = n()
  ) 

# Puede comprobarse que hay más gvkey_iid que isin
DatosMX01 %>%
  summarise_all(n_distinct)
```

Las tablas anteriores muestran que cada código "GVKEY-IID" está asociado a un único "isin". 
Hay varios códigos "GVKEY-IID" en  el que"isin" es "NA", entonces . Revisamos las características de este conjunto de datos.

```{r}
DatosMX01 %>%
  group_by(gvkey_iid) %>%
  filter(is.na(isin)) 

```

Aún cuando el "isin" es "NA", los registros tienen información de precios y otra información relevante. Al estar identificados con "gvkey" y "iid", no es relevante que no contengan "isin".

<p style="color:red">**NOTA: Debe notarse que un "gvkey" está asociado a un nombre de empresa. Esto implica que si sólo analizamos los "gvkey" encontraremos fechas duplicadas. Pero no sería adecuado eliminar los registros con fechas duplicadas, pues pueden corresponder a una emisión ("iid") diferente.**</p>

Verificamos entonces cuáles son los `gvkey_iid` que tienen alguna fecha duplicada.
```{r}
# Cuales son el combinado de fecha/emisora que aparecen en la base más de una vez  y los extraemos hacia un vector
get_dupes(DatosMX01, gvkey_iid, datadate)
```
No hay fechas duplicadas.

## Precio ajustado y capitalización de mercado.

Para calcular los precios ajustados, primero revisamos si existen precios(prccd) negativos y luego realizamos los cálculos correspondientes.

```{r}

# Para los registros previos al 1 de enero de 1993, dividimos prccd entre 1000 para convertirlos en MXN.
# Entonces, calculamos el Precio ajustado.
# Calculamos el Valor de Mercado
DatosMX01 <- DatosMX01 %>%
  mutate(
    prccd = ifelse(curcdd == "MXP", prccd / 1000, prccd),
    adj_price = (prccd / ajexdi) * trfd,
    mv = prccd * cshoc / 1000000
  )

```

## Selección por Valor de Mercado.

```{r}
# de los gvkey que tienen mas de una emisión... cual es el valor de mercado promedio de cada emisión por empresa, se pone en un vector la emisión que más valor de mercado tenga para quedarse con dicha emisión...

keep_emisoras <- DatosMX01 %>% 
  group_by(gvkey, iid) %>%
  summarise(prom_mv = mean(mv, na.rm = TRUE)) %>% 
  filter(n_distinct(iid) > 1) %>% 
  top_n(1, prom_mv) %>% 
  mutate(gvkey_iid = paste(gvkey, iid, sep = "_")) %>% 
  pull(gvkey_iid) 

keep_emisoras <- c(
  keep_emisoras,
  DatosMX01 %>%
    group_by(gvkey, iid) %>%
    summarise(prom_mv = mean(mv, na.rm = TRUE)) %>%
    filter(n_distinct(iid) == 1) %>%
    mutate(gvkey_iid = paste(gvkey, iid, sep = "_")) %>%
    pull(gvkey_iid)
)

### CHECKPOINT ###
#Estos serían los registros que nos quedarían
DatosMX01 <- DatosMX01 %>%
  filter(gvkey_iid %in% keep_emisoras) %>% 
  arrange(datadate, gvkey) 

one_record_filt <- DatosMX01 %>% 
  count(gvkey_iid) %>% 
  filter(n == 1) %>% 
  pull(gvkey_iid)

DatosMX01 <- DatosMX01 %>% 
  filter(!gvkey_iid %in% one_record_filt)
```

## Filtros de fin de mes.
Enseguida vamos a generar dos data frames.
Uno contiene a las observaciones que son fin de mes (DatosMX02), 
y el otro al resto de observaciones (DFnoFinMes).
Del segundo de estos data frames, identificaremos aquellos "gvkey_iid" + "año-mes" que no tienen un identificador igual en el data frame de fin de mes; estas observaciones las utilizaremos para insertarlas en DatosMX02.

```{r}
# Generamos entonces el data frame que tiene las observaciones que SÍ son "fin de mes". 
datos_mensuales <- DatosMX01 %>% filter(monthend == 1)

# Generamos ahora el data frame con las observaciones que no son fin de mes, hasta la ejecución de estas instrucciones, la suma de estas observaciones y las observaciones del filtro anterior resultan en el total de observaciones en "DatosMX01".
DFnoFinMes <- DatosMX01 %>% filter(monthend != 1)
```

En el data frame que contiene los registros que "NO son fin de mes", mantenemos las observaciones correspondientes a la última fecha de cada mes.
Ref: https://stackoverflow.com/questions/55933064/last-observation-for-each-company-for-each-month-r
```{r}
DFnoFinMes <- DFnoFinMes %>%
  mutate(month = yearmonth(datadate)) %>% 
  group_by(
    gvkey_iid,
    month
  ) %>%
  slice(
    which.max(
      datadate 
    )
  ) %>%
  ungroup() %>%
  select(-month)
```

En el data frame que contiene los registros que "NO son fin de mes", para cada "gvkey_iid" + "año-mes", identificamos los meses que sí están en "DatosMX02". Mantenemos sólo las observaciones "gvkey_iid" + "año-mes" que no están en "DatosMX02".

Ref: https://stackoverflow.com/questions/51215628/how-to-perform-an-anti-join-or-left-outer-join-get-all-the-rows-in-a-dataset 
```{r}
DFnoFinMes <- DFnoFinMes %>%
  mutate(
    tempID = paste(
      gvkey_iid,
      format(datadate, "%Y-%m"),
      sep = "_"
    )
  ) %>%
  anti_join(
    (datos_mensuales %>%
      mutate(
        tempID = paste(
          gvkey_iid,
          format(datadate, "%Y-%m"),
          sep = "_"
        )
      )
    ),
    by = "tempID"
  ) %>%
  select(-tempID)

# Finalmente, unimos las observaciones de DFnoFInMes a DatosMX02.
datos_mensuales <- union_all(datos_mensuales, DFnoFinMes)
```

# Rellenado de todas las empresas

```{r}
datos_mensuales <- datos_mensuales %>% mutate(
  year = year(datadate),
  mes = yearmonth(datadate)
)

datos_mensuales <- datos_mensuales %>%
  as_tsibble(
    key = gvkey_iid,
    index = mes
  ) %>%
  fill_gaps() %>%
  fill(-datadate, -year, .direction = "down") %>%
  mutate(
    datadate = if_else(
      is.na(datadate),
      make_date(
        year = year(mes),
        month = month(mes),
        day = days_in_month(month(mes))
      ),
      datadate
    ),
    year = year(mes)
  ) %>% 
  arrange(datadate)
```

## Creación de rendimientos.. Alternativa uno
```{r}
sectores_list <- datos_mensuales %>% 
  select(gvkey_iid, datadate, adj_price, BMV, mes) %>% 
  group_by(gvkey_iid) %>% 
  mutate(returns = (adj_price / lag(adj_price)) - 1) %>% 
  group_by(BMV) %>% 
  #index_by(mes) %>% 
  group_split()

sectores_list <- set_names(sectores_list, map_chr(sectores_list, ~ .x$BMV[[1]]))
  
  
returns_list <- sectores_list %>%
  modify(~ index_by(.x) %>% summarise(pf_rets = mean(adj_price)))

```

## Rendimientos Equally weighted

```{r}

lagged_returns <- function(var) {
  var <- enquo(var)
  indices <- c(12, 24, 60)
  map(indices, ~ quo(!!var / lag(!!var, !!.x) - 1)) %>%
    set_names(sprintf("Return_%02d", indices))
}

returns_sector_ew <- datos_mensuales %>% 
  group_by(gvkey_iid) %>% 
  select(gvkey_iid, datadate, conm, BMV, adj_price, mv, mes) %>% 
  mutate(!!!lagged_returns(adj_price)) %>% 
  group_by(BMV) %>% 
  arrange(conm, datadate) %>% 
  group_split()
  
returns_sector_ew <- set_names(returns_sector_ew, map_chr(returns_sector_ew, ~ .x$BMV[[1]]))

#
returns_sector_ew %>%
  map(
    ~ index_by(.x) %>%
      select(gvkey_iid, mes, Return_12) %>%
      pivot_wider(
        names_from = gvkey_iid,
        values_from = Return_12
      )
  ) %>%
  `[[`(1) %>%
  ungroup() %>% 
  arrange(mes) %>% 
  slice(365) %>%
  select(-1) %>% 
  map_dbl(~.x) %>% mean(na.rm=T)


returns_sector_ew1 <- returns_sector_ew %>%
  map(~ index_by(.x) %>% summarise(
    sec_ret_12 = mean(Return_12, na.rm = T),
    sec_ret_24 = mean(Return_24, na.rm = T),
    sec_ret_60 = mean(Return_60, na.rm = T)
  ))
# RENDMIENTOS Y RUN UPS

```

